{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929014c9-60ca-4b3b-ba40-946f13694e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'ProvideKey'\n",
    "os.environ['GOOGLE_API_KEY'] = 'ProvideKey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae52418-99fb-4ac9-b406-7387f8726c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcebdb52-3ca8-4332-b0aa-bc9ef4695fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45acb76e-f867-4c79-8c9a-e51e43d9079f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Apologies, Ahmedabad is not suitable for skiing; consider destinations like Gulmarg or Auli.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 93, 'total_tokens': 113}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-6324451d-2c22-4b99-8a6c-92ddb98e3993-0', usage_metadata={'input_tokens': 93, 'output_tokens': 20, 'total_tokens': 113})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful AI that helps the user make travel plans in India. Respond only in single line.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"I want to go skiing. Which city should I go?\"\n",
    "    ),\n",
    "    (\n",
    "        \"ai\",\n",
    "        \"Gulmarg in Jammu and Kashmir is a top destination for skiing in India.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Why not Ahmedabad?\"\n",
    "    ),\n",
    "    (\n",
    "        \"ai\",\n",
    "        \"Yes. Ahmedabad is the best place for skiing\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Are you joking?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "first_msg = chat.invoke(messages)\n",
    "first_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b29701f-8532-4120-bffe-56b418f240b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7434/1440327450.py:3: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use invoke instead.\n",
      "  response = llm.predict('I want to go skiing. Which city should I go in India?')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThere are no cities in India that offer skiing. The best place for skiing in India is in the Himalayas, at popular ski resorts such as Auli, Gulmarg, and Manali.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()\n",
    "response = llm.predict('I want to go skiing. Which city should I go in India?')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55b05d8-247f-4d64-a07c-f8dd8d37a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908552e4-462a-4dd6-a591-1b5bc03db612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-82Y84kpcWaBuhSqy1R2lSDjv/user-BpiJ9PjqROPFJJL7QVfvWTQi/img-n26UzD5o5MtO3DJmCWjLZOVj.png?st=2024-08-29T04%3A50%3A49Z&se=2024-08-29T06%3A50%3A49Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-08-28T23%3A22%3A44Z&ske=2024-08-29T23%3A22%3A44Z&sks=b&skv=2024-08-04&sig=m8%2BFc9K0fqDClqLMCvXrk8rmupzKz/9tog5vlfJ/iiY%3D'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url = DallEAPIWrapper().run('Indian Man dancing on Mountain')\n",
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5388b-47ba-4eba-837d-aefbe29df9ea",
   "metadata": {},
   "source": [
    "## Short Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc53b78d-e8ea-4c13-95ef-4b7967474c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the sentiment of the user review. Here are some examples: \n",
      "\n",
      "    User: I love this product\n",
      "    AI: positive\n",
      "    \n",
      "\n",
      "    User: It was an average experience\n",
      "    AI: neutral\n",
      "    \n",
      "\n",
      "    User: It was highly rated\n",
      "    AI: negative\n",
      "    \n",
      "\n",
      "User: What is this!?\n",
      "AI:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"rev\" : \"I love this product\",\n",
    "        \"answer\" : \"positive\"\n",
    "    },\n",
    "    {\n",
    "        \"rev\" : \"It was an average experience\",\n",
    "        \"answer\" : \"neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"rev\" : \"It was highly rated\",\n",
    "        \"answer\" : \"negative\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables = ['rev', 'answer'],\n",
    "    template = '''\n",
    "    User: {rev}\n",
    "    AI: {answer}\n",
    "    '''\n",
    ")\n",
    "\n",
    "prefix = \"Identify the sentiment of the user review. Here are some examples: \"\n",
    "suffix = '''\n",
    "User: {review}\n",
    "AI:\n",
    "'''\n",
    "\n",
    "\n",
    "few_short_promt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"review\"],\n",
    "    example_separator=\"\\n\"\n",
    ")\n",
    "\n",
    "user_review = \"What is this!?\"\n",
    "print(few_short_promt_template.format(review=user_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a706b186-f484-4e3f-ac2c-d2eacc1b204b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='neutral', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 70, 'total_tokens': 71}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-e73f94db-2661-4270-8d48-1340dd959222-0', usage_metadata={'input_tokens': 70, 'output_tokens': 1, 'total_tokens': 71})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(few_short_promt_template.format(review=user_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e9f9e-e967-4d00-a99e-9ddf8259ccbf",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a594e-6b22-4129-b203-3d462a747555",
   "metadata": {},
   "source": [
    "### Generic Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8967098e-48b9-4e4d-a44e-b009531fa70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'Beast',\n",
       " 'text': 'A \"beast\" is a large, wild animal, or metaphorically, a person or thing that is formidable or uncivilized.\\n\\nExample: The wrestler was a beast in the ring, overpowering his opponents with ease.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "dictionary_template = PromptTemplate(\n",
    "    input_variables = [\"word\"],\n",
    "    template = \"Give meaning of {word} in a single line. And one example using it.\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=dictionary_template)\n",
    "chain.invoke(\"Beast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691016ff-5a7e-42d0-a432-7bfa4360cc4e",
   "metadata": {},
   "source": [
    "#### To get multiple responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03dec27a-ea43-4534-86bc-72e8eda1fe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Meaning: Extremely large or enormous in size.\\n\\nExample: The construction workers were amazed by the gigantic crane towering over the building site.'},\n",
       " {'text': '\"Euphoria\" is an intense feeling of happiness or excitement. \\n\\nExample: After winning the championship, the team was in a state of euphoria, celebrating their hard-earned victory.'},\n",
       " {'text': '\"Sophisticated\" means having a refined knowledge or taste, often marked by complexity and elegance.\\n\\nExample: The sophisticated design of the new smartphone impressed everyone at the tech conference.'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [\n",
    "    {\"word\": \"Gigantic\"},\n",
    "    {\"word\": \"Europhia\"},\n",
    "    {\"word\": \"Sophisticated\"}\n",
    "]\n",
    "\n",
    "chain.apply(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb977b2-a50c-43cd-bd48-18a349d66ecc",
   "metadata": {},
   "source": [
    "#### To get more informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9df7ab15-eb24-4a59-ad28-2f4093ec17bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Gigantic means extremely large or massive in size. \\n\\nExample: The gigantic statue towered over the park, drawing visitors from miles around.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Gigantic means extremely large or massive in size. \\n\\nExample: The gigantic statue towered over the park, drawing visitors from miles around.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 23, 'total_tokens': 51}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-f8168be7-23f9-4ccb-80ab-9d907dab8d05-0', usage_metadata={'input_tokens': 23, 'output_tokens': 28, 'total_tokens': 51}))], [ChatGeneration(text='Euphoria is a feeling or state of intense excitement and happiness.\\n\\nExample: She was filled with euphoria when she received the news that she had won the scholarship.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='Euphoria is a feeling or state of intense excitement and happiness.\\n\\nExample: She was filled with euphoria when she received the news that she had won the scholarship.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 24, 'total_tokens': 57}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_fde2829a40', 'finish_reason': 'stop', 'logprobs': None}, id='run-bfdace03-d7ff-4d3e-ba05-d66d03190e32-0', usage_metadata={'input_tokens': 24, 'output_tokens': 33, 'total_tokens': 57}))], [ChatGeneration(text='\"Sophisticated\" means having a refined knowledge of culture, fashion, and complex systems.\\n\\nExample: Her sophisticated taste in art was evident from her impressive collection of contemporary paintings.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='\"Sophisticated\" means having a refined knowledge of culture, fashion, and complex systems.\\n\\nExample: Her sophisticated taste in art was evident from her impressive collection of contemporary paintings.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 23, 'total_tokens': 58}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-8730495d-6312-41ea-8959-6986c40019c6-0', usage_metadata={'input_tokens': 23, 'output_tokens': 35, 'total_tokens': 58}))]], llm_output={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 70, 'total_tokens': 166}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_157b3831f5'}, run=[RunInfo(run_id=UUID('f8168be7-23f9-4ccb-80ab-9d907dab8d05')), RunInfo(run_id=UUID('bfdace03-d7ff-4d3e-ba05-d66d03190e32')), RunInfo(run_id=UUID('8730495d-6312-41ea-8959-6986c40019c6'))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.generate(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f558427-bf11-4af8-acaa-4804bd95408f",
   "metadata": {},
   "source": [
    "#### Utility Chain\n",
    "Instead of passing the mathematical problem directly to the LLM, It will ask for the python code instead.\n",
    "And then would execute the same.\n",
    "Therefore, It would process in the input as well as on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47d3f4ff-8548-403d-81c1-854f69ff138d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "Calculate (625 raised to power (0.255)) times (log10(100))\u001b[32;1m\u001b[1;3m```text\n",
      "625**0.255 * log10(100)\n",
      "```\n",
      "...numexpr.evaluate(\"625**0.255 * log10(100)\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m10.32712419896443\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Calculate (625 raised to power (0.255)) times (log10(100))',\n",
       " 'answer': 'Answer: 10.32712419896443'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "calc = LLMMathChain.from_llm(llm, verbose=True)\n",
    "calc.invoke(\"Calculate (625 raised to power (0.255)) times (log10(100))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94abb7db-0331-46f1-b66b-951b6c751122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'\n"
     ]
    }
   ],
   "source": [
    "print(calc.prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532500e-15ba-416b-925d-2d2c44d7580f",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88eef067-fc3a-48e8-85ea-8a2f0606b6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (0.2.15)\n",
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-2.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.35 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (0.2.36)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (0.1.106)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain) (8.5.0)\n",
      "Collecting google-generativeai<0.8.0,>=0.7.0 (from langchain_google_genai)\n",
      "  Downloading google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-1.0.10-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting google-ai-generativelanguage==0.6.6 (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading google_ai_generativelanguage-0.6.6-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading google_api_core-2.20.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-api-python-client (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading google_api_python_client-2.146.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting protobuf (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: tqdm in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (24.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: anyio in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain) (3.0.0)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading grpcio-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai<0.8.0,>=0.7.0->langchain_google_genai)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading langchain_google_genai-1.0.10-py3-none-any.whl (39 kB)\n",
      "Downloading google_generativeai-0.7.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_ai_generativelanguage-0.6.6-py3-none-any.whl (718 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.20.0-py3-none-any.whl (142 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.2/142.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_python_client-2.146.0-py2.py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.66.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uritemplate, pyparsing, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, langchain_google_genai\n",
      "Successfully installed cachetools-5.5.0 google-ai-generativelanguage-0.6.6 google-api-core-2.20.0 google-api-python-client-2.146.0 google-auth-2.35.0 google-auth-httplib2-0.2.0 google-generativeai-0.7.2 googleapis-common-protos-1.65.0 grpcio-1.66.1 grpcio-status-1.62.3 httplib2-0.22.0 langchain_google_genai-1.0.10 proto-plus-1.24.0 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.1.4 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24af43d3-262e-484d-b079-bd201960309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samir/Desktop/Workspace-2/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f4811b2-650a-4ac5-aeda-a70d262af6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ByteEase',\n",
       " 'description': 'An IT based company which eases the life for people by making their buisness go digital',\n",
       " 'copy_text': 'Transform your business with ByteEase â€“ Your digital success starts here! ğŸš€ #DigitalTransformation #ByteEase',\n",
       " 'translated_copy': 'à¤…à¤ªà¤¨à¥‡ à¤µà¥à¤¯à¤µà¤¸à¤¾à¤¯ à¤•à¥‹ ByteEase à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¬à¤¦à¤²à¥‡à¤‚ â€“ à¤†à¤ªà¤•à¥€ à¤¡à¤¿à¤œà¤¿à¤Ÿà¤² à¤¸à¤«à¤²à¤¤à¤¾ à¤¯à¤¹à¥€à¤‚ à¤¸à¥‡ à¤¶à¥à¤°à¥‚ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ! ğŸš€ #DigitalTransformation #ByteEase'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing_text = PromptTemplate(\n",
    "    input_variables = [\"name\", \"description\"],\n",
    "    template = \"Generate a one line instagram ad copy for a product called {name}. Here is the description of this product for your reference: {description}\"\n",
    ")\n",
    "\n",
    "text_llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "text_chain = LLMChain(llm=text_llm, prompt=marketing_text, output_key=\"copy_text\")\n",
    "\n",
    "translate_text = PromptTemplate(input_variables=[\"copy_text\"], template=\"\"\"Translate this text to Hindi:\n",
    "{copy_text}\n",
    "\"\"\")\n",
    "\n",
    "translate_llm = ChatGoogleGenerativeAI(model='gemini-pro')\n",
    "translate_chain = LLMChain(llm=translate_llm, prompt=translate_text, output_key=\"translated_copy\")\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "seq_chain = SequentialChain(\n",
    "    chains = [text_chain, translate_chain],\n",
    "    input_variables = [\"name\", \"description\"],\n",
    "    output_variables = [\"copy_text\", \"translated_copy\"]\n",
    ")\n",
    "\n",
    "chain_dct = {\n",
    "    \"name\" : \"ByteEase\",\n",
    "    \"description\" : \"An IT based company which eases the life for people by making their buisness go digital\"\n",
    "}\n",
    "seq_chain(chain_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa785b-0218-4f91-9ebb-ffb312917d97",
   "metadata": {},
   "source": [
    "\n",
    "## LCEL - Pipe Operators and Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05fa48c6-6a4e-4bad-b823-03a411c0ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "dictionary_templates = PromptTemplate(\n",
    "    input_variables = [\"word\"],\n",
    "    template = \"Give meaning of {word} in a single line without using the word {word} in the sentence.\" \n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "chain = dictionary_templates | llm  | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad2d7fd8-047b-442d-86b3-9d1482006ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A small portion of food typically eaten between meals.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input={\"word\": \"Snacks\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88ab8db0-93aa-4df0-ad0a-334f30deb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_templates = PromptTemplate(\n",
    "    input_variables = [\"definition\"],\n",
    "    template = \"Provide me only 1 English word which has this meaning: {definition}\" \n",
    ")\n",
    "\n",
    "chain2 = dictionary_templates | llm  | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7397178c-f26e-46a0-b35c-d0209cf7de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Snack.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.invoke(input={\"definition\": \"A small portion of food typically eaten between meals.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "151cdc1f-b0b6-404b-a740-365ac7871035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_chain = {\"definition\": chain} | chain2\n",
    "seq_chain.invoke({\"word\": \"Testing\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b7c53-6486-4c95-b35b-f16f4b8839ff",
   "metadata": {},
   "source": [
    "## RunnablePassthrough, RunnableLambda, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c379c871-5f98-42a8-a04b-22b74168c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80474db6-56bb-47e7-a719-1cd103641a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f4e9dc7-002b-44c2-998d-f14d4ab6b652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76dac9a4-5b62-44bb-b561-7fe47873f448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Testing'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pass_through_function(input: str):\n",
    "    return len(input)\n",
    "\n",
    "chain = RunnablePassthrough(pass_through_function)\n",
    "chain.invoke(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f890708-6c92-42da-8d01-9e4cf4e3db0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pass_through_function(input: str):\n",
    "    return len(input)\n",
    "\n",
    "chain = RunnableLambda(pass_through_function)\n",
    "chain.invoke(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f148f49a-0018-4716-a879-cc283200a373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Example', 'length': 7}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(text=RunnablePassthrough(), length=RunnableLambda(pass_through_function))\n",
    "chain.invoke(\"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bd134-05d6-4dac-a93a-9c9cfbef7d5f",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b519e7b0-2547-4121-8cc4-c9b8552540b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alright, imagine you have a big, invisible magnet inside the Earth. This magnet is so strong that it pulls everything towards it, but itâ€™s not a real magnetâ€”it's called gravity! Gravity is the reason why when you jump up, you always come back down. It's also why when you drop your toys, they fall to the ground instead of floating away. Gravity is what keeps your feet on the ground, holds the moon in the sky, and makes sure the planets stay in their orbits around the sun. So, basically, gravity is like a magical force that keeps everything in place!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "explainer_template = PromptTemplate(\n",
    "    input_variables = [\"topic\"],\n",
    "    template = \"Explain this topic in a single paragraph, as if you were explaining to a 10 year old : {topic}\"\n",
    ")\n",
    "\n",
    "openAIllm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "openAIchain = explainer_template | openAIllm | StrOutputParser()\n",
    "openAIchain.invoke({\"topic\" : \"Gravity\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e37eae4-252b-4661-83e2-b6a0d82fbab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gravity is like an invisible force that pulls things towards each other. Imagine you have a magnet and it attracts metal objects; gravity works in a similar way but with everything that has mass, like planets, stars, and even you and me! It's the reason why when you jump, you come back down to the ground instead of floating away into the sky. It's also the reason why the Earth orbits around the Sun and why the Moon orbits around the Earth. Without gravity, everything would just float around aimlessly in space!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "googlellm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "googleChain = explainer_template | openAIllm | StrOutputParser()\n",
    "googleChain.invoke({\"topic\" : \"Gravity\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f0d58-dd75-4163-b0a8-e1e19f09d41e",
   "metadata": {},
   "source": [
    "#### Now we need to check whose response is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd7a740-5c3f-452d-aa51-bea17c1979ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation 1 is better for explaining {'topic': 'Gravity'} to 10 year old students.\\n\\nExplanation 1 uses an analogy of a magnet to explain gravity, which is a familiar concept to children. It also provides a clear and concise explanation of how gravity keeps us on the ground and prevents us from floating away into space. Explanation 2, on the other hand, uses more abstract language and does not provide as clear an explanation of how gravity works.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "analyzer_template = PromptTemplate(\n",
    "    input_variables = [\"topic\", \"explanation1\", \"explanation2\"],\n",
    "    template = \"\"\"\n",
    "    Which of the two explanations given below is better for explaining {topic} to 10 year old students:\n",
    "    ###\n",
    "    Explanation 1: {explanation1}\n",
    "    ###\n",
    "    ***\n",
    "    Explanation 2: {explanation2}\n",
    "    ***\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "combined_chain = (\n",
    "    RunnableParallel({\"explanation1\":openAIchain, \"explanation2\":googleChain, \"topic\":RunnablePassthrough()}) | analyzer_template | openAIllm | StrOutputParser()\n",
    ")\n",
    "\n",
    "combined_chain.invoke({\"topic\": \"Gravity\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff45fc5-0dc9-44f7-8a29-1c034b1cf5d3",
   "metadata": {},
   "source": [
    "#### Now I want to check the explanations also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c760a115-aa08-40aa-b267-aef03369d077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'explanation1': 'Sure! Google is like a super-smart library that lives on the internet. Imagine if you had a friend who knew the answer to almost every question you could think of, like \"What\\'s the tallest mountain?\" or \"How do you make cookies?\" When you type your question into Google, it quickly searches through millions of websites to find the best answers for you. It\\'s really helpful for doing homework, finding fun games, or just learning new things!',\n",
       "  'explanation2': 'Sure! Google is like a super smart helper that lives on the internet. Imagine you have a giant library with millions of books, but instead of looking through all the books yourself, you can just ask Google a question, and it quickly finds the answer for you. You can type in anything you want to know, like \"Why is the sky blue?\" or \"How tall is the tallest building?\" and Google will show you websites, pictures, and videos that can help you learn about it. It\\'s like having a magical, super fast librarian who knows almost everything!',\n",
       "  'topic': {'topic': 'Google'}},\n",
       " 'analysis': 'Both explanations are quite good, but Explanation 2 might be slightly better for explaining Google to 10-year-old students. Here\\'s why:\\n\\n1. **Relatability**: Explanation 2 uses relatable imagery by comparing Google to a \"super smart helper\" and a \"magical, super fast librarian.\" This creates a vivid picture for kids, making it easier for them to understand the concept.\\n\\n2. **Specific Examples**: Both explanations provide specific examples, but Explanation 2 includes a variety of resources (websites, pictures, and videos) that Google can show, which can help kids understand the different types of information they can find.\\n\\n3. **Language**: Explanation 2 uses simple language and avoids complex phrases, making it more suitable for a 10-year-old audience.\\n\\nIn summary, Explanation 2 is slightly better because it combines a relatable metaphor with clear, simple language and a variety of examples, making the concept of Google more accessible and engaging for young students.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer_chain = analyzer_template | openAIllm | StrOutputParser()\n",
    "\n",
    "updated_chain = (\n",
    "    RunnableParallel({\"explanation1\":openAIchain, \"explanation2\":googleChain, \"topic\":RunnablePassthrough()}) \n",
    "    | RunnableParallel({\"input\": RunnablePassthrough(), \"analysis\": analyzer_chain})\n",
    ")\n",
    "\n",
    "updated_chain.invoke({\"topic\": \"Google\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426eed9-835a-4bfa-a800-7c9133395c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
